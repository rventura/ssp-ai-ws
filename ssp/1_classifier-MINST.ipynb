{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2989a2a-a28d-435c-9dfc-39877af0962f",
   "metadata": {},
   "source": [
    "# Classification of handwritten digits\n",
    "\n",
    "This notebook demonstrates the use of simple neural networks to classify handwritten digits.\n",
    "\n",
    "It uses the well known MNIST dataset, comprising 70,000 greyscale scans of handwritten digits of 28x28 pixels. This dataset has been widely used, both as a benchmark dataset, and for educational purposes. See https://en.wikipedia.org/wiki/MNIST_database for more information.\n",
    "\n",
    "We will train a machine learning model, in this case a neural network, for a classification task of the MNIST digits. That is, the input of the model is an image and the output is a label, i.e., the corresponding digit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d90d372-769f-4101-9aba-7d840839b4ef",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll use\n",
    "* time -- a standard Python library providing time related functions\n",
    "* matplotlib for showing images -- more info in https://matplotlib.org/\n",
    "* PyTorch is a machine learning library -- more info in https://pytorch.org/\n",
    "  * torch.nn for using neural networks\n",
    "  * torch.utils.data for loading datasets\n",
    "  * torchvision.datasets for access to the MNIST dataset\n",
    "  * torchvision.transforms for data transformation among images and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f49ce7-6f3e-4bd3-99bb-d73a854d0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037605d3-89da-4eee-91dd-84c3e141130a",
   "metadata": {},
   "source": [
    "Determine if there is any AI acceleration in the machine. Otherwise, use the CPU. Typical values are \"cuda\" for NVIDIA GPUs and \"mps\" for Apple Sillion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2f8ee-48b4-47d5-a848-19b5c7bd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1562e2be-e357-4548-90c9-7f532888dd9e",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "\n",
    "Next we load the MNIST dataset and transform it into a sequence of tensors. Tensors are the basic data type employed by pyTorch.\n",
    "\n",
    "At the first time, it will download the dataset from the internet, otherwise it will use the previously downloaded one.\n",
    "\n",
    "Then, split the dataset into three subsets:\n",
    "* __train set__ -- used for training the model (70% in this example)\n",
    "* __test set__ -- used for evaluating the model on instances unseen during training (15%)\n",
    "* __validation set__ -- used to prevent overfitting during training (not used here, 15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40905067-9999-4bbc-a2f7-ac8dec4e252d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "full_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Get the dataset size in number of instances\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    full_dataset, [int(0.7*dataset_size),\n",
    "                   int(0.15*dataset_size),\n",
    "                   int(0.15*dataset_size)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fc9d38-da7f-4624-9a8a-9549d2e3edbe",
   "metadata": {},
   "source": [
    "In training (but also during testing), data is split into __batches__. The model weights are only updated after each batch.\n",
    "\n",
    "Next we define the batch size and the dataloaders necessary for what follows. Dataloaders will divide data into batches. Small batches introduce more noise during learning, which may help escaping from local minima, but might slow down convergence. Large batches tipically speeds up training but may lead to overfitting more easily.\n",
    "\n",
    "It also shows the dimensions of one batch from the test dataset, both the input (x) and the output (y). The dimensions are: batch size, number of channels (1), and width and height or each images (28 by 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0b60a-299f-4f3a-be27-f1bf6fa4b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line defined the batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader      = DataLoader(train_dataset, batch_size=batch_size)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size)\n",
    "test_dataloader       = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Show the shape of the first instance from the test dataset\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9f9e7-2998-4a61-822f-339ad52930a5",
   "metadata": {},
   "source": [
    "Here we show some examples from the three sets, one row per set: train, validation, and test. On top of each images is the label in the dataset (the groundtruth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c8ea7-4d8a-4d6b-9455-581ba1865502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(fig,ax) = plt.subplots(3, 10)\n",
    "for i,ds in enumerate([train_dataset, validation_dataset, test_dataset]):\n",
    "    for j in range(10):\n",
    "        ax[i,j].imshow(ds[j][0][0], cmap=\"gray\")\n",
    "        ax[i,j].set_axis_off()\n",
    "        ax[i,j].set_title(ds[j][1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b3227-e483-42c2-a389-2f409267b869",
   "metadata": {},
   "source": [
    "## Create a neural network model\n",
    "\n",
    "The next cells defines several neural network models. All networks have 28*28 inputs and 10 outputs. Each output corresponds to a label, among the 10 digits. The classification label will be the output with greatest value.\n",
    "\n",
    "The first one, __ClassicMNIST__, is a simple fully connected model. There are several alternatives, from a simple linear network, to 2 hidden layers. The activation function for the hidden layers is a ReLU, meaning rectifier linear unit, where the output is equal to the input if it is non-negative, and zero otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3a115-f2e8-41f0-a24d-7f59e07f7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # linear network\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(28*28, 10)\n",
    "        )\n",
    "\n",
    "        # 1 hidden layer\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(28*28, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "        # 2 hidden layers\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(28*28, 40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(40, 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        # Edit the next line to define which of the above neural networks to use\n",
    "        logits = self.hidden2(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11738d9-18ae-4a50-a86b-080c2e42c8d1",
   "metadata": {},
   "source": [
    "__SmallMNISTCNN__ is a model comprising: (1) two CNN layers, each one with batch normalization, ReLU activation functions, and a max pooling layer, and (2) two fully connected layers, with a ReLU and a dropout in between them.\n",
    "\n",
    "A CNN (convoluional neural network) is an architecture where weights are shared among several receptive fields, somehow mimicking animal visual cortex.\n",
    "\n",
    "Batch normalization layers helps stabilizing training by normalizing its inputs to zero mean and unit variance over each batch.\n",
    "\n",
    "Max pooling layers aggregate data by choosing the maximun value among groups of inputs (receptive fields).\n",
    "\n",
    "Dropout layers randomly deactivates some neurons during training, forcing learning to more distributed among the network and helps preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0976556-9ea9-45c9-a1a4-9cec2405c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallMNISTCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(32*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(128, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2cd801-4bdb-44ae-9743-5c42ce9256e0",
   "metadata": {},
   "source": [
    "Next we define a __LeNet5__ model, based on a well-known CNN model created by the pioneer Yann LeCun back in 1998. CNN are a cornerstone of any modern state-of-the-art deep learming model for images and similar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3798dc3-1af2-419e-98ef-3e88b04e8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2),\n",
    "            #\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f818330-f3a1-4519-883a-f626d742d2c4",
   "metadata": {},
   "source": [
    "Next we instantiate the model and transfer it to the AI accelerator (if any). The structure of the model is printed.\n",
    "\n",
    "We also define the loss function (the cross entropy loss) and the optimization algorithm (Adam).\n",
    "\n",
    "The cross entropy loss, a information theoretic concept, promotes accurate answers.\n",
    "\n",
    "Adam optimizer is a widely used optimization methods. It uses backpropagation to compute the gradient of the loss function with respect to the weights, and makes a step towards its descent, plus a momentum term to make the gradient descent faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8c3d5-eb6a-470f-99a8-879ed3687170",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the model you want to try out, leaving all others commented out\n",
    "\n",
    "model = ClassicMNIST().to(device)\n",
    "#model = SmallMNISTCNN().to(device)\n",
    "#model = LeNet5().to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1785f3cd-6c53-40ab-b0e5-8af189366e0c",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We start by defining the train and the test functions.\n",
    "\n",
    "The train function iterates over all batches of the training set. For each batch, computes the prediction error (loss) and __backpropagates__ the error through the network and adjusts the network parameters towards decreasing the loss. This is known as stochastic gradient descent, since at each step, the batch is different from the previous one. This helps escaping from local minima.\n",
    "\n",
    "Besides printing the start of each epoch, it also prints the loss in 1 second intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a69d5-4d56-4ac9-9bc6-21f854d912a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    t = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prints loss progress every second\n",
    "        if time.monotonic()-t > 1:\n",
    "            t = time.monotonic()\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a6a1d-5bee-4058-b0b5-1921b32b2c37",
   "metadata": {},
   "source": [
    "In the test function, go though all batches in the test dataset, computing the total loss, and the amount of correctly classified instances. It prints the accuracy and the average loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f59907-f674-4ee1-a7ab-20a67bf41049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ffc5c2-7511-44b6-b0a9-eaffb00157a6",
   "metadata": {},
   "source": [
    "Here we actually perform the training of the model instantiated above.\n",
    "\n",
    "The first line defines the number of epochs, that is, how many times training goes through all batches in the training set.\n",
    "\n",
    "The \"for\" loop iterates over all epochs, outputing the performance in both the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afb773-8fd9-414a-ab9f-f47b27bb8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define here the number of epochs. Increase if loss descent does not stabilize in time.\n",
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(\"Validation: \", end=\"\")\n",
    "    test(validation_dataloader, model, loss_fn)\n",
    "    print(\"Test: \", end=\"\")\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6060d-7099-4660-a6cb-013d18d7bc5a",
   "metadata": {},
   "source": [
    "Use this cell __only__ if you wish to save the trained model to a file (otherwise, just skip it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52460cc-087f-4b2e-91fa-2312b06e52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec03033-2b7e-4051-b1d1-d1fa4ef85185",
   "metadata": {},
   "source": [
    "Use this cell __only__ if you wish to loadm a model from a file (otherwise, just skip it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a959a-2935-457d-ac3b-d7fa090ed669",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0bffe-52df-4e9c-a15b-1c002e5912f5",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "The next cell shows classification of some of the instances from the dataset.\n",
    "\n",
    "As before, three rows are shown, one for each one of the train, validation, and test sets. At the top of each image, the first number is the __classification result__ and the second number, in paranthesis, is the __groundtruth__ (that is, the true value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c7e52-8fff-4bf6-9b87-20807f58ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "(fig,ax) = plt.subplots(3, 10)\n",
    "for i,ds in enumerate([train_dataset, validation_dataset, test_dataset]):\n",
    "    for j in range(10):\n",
    "        x, y_gt = ds[j]\n",
    "        x = x[None,:].to(device)\n",
    "        y_out = model(x)\n",
    "        pred = y_out[0].argmax(0)\n",
    "        \n",
    "        ax[i,j].imshow(ds[j][0][0], cmap=\"gray\")\n",
    "        ax[i,j].set_axis_off()\n",
    "        ax[i,j].set_title(f\"{int(pred)} ({y_gt})\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed6979c-2882-40a5-825c-a673dff80fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
