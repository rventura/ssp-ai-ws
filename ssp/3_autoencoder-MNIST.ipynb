{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e1ebcc-fd7d-4247-bcdd-7263dbeda0c2",
   "metadata": {},
   "source": [
    "# Unsupervised learning using the MNIST dataset\n",
    "\n",
    "This notebook explores the use of an __autoencoder__ neural network.\n",
    "\n",
    "An autoencoder features two modules in sequence: and __encoder__ and a __decoder__. The encoder maps an input layer to a smaller layer, called the __bottleneck__. Them, the decoder maps the bottleneck into an output layer.\n",
    "\n",
    "The training adjusts both encoder and decoder weights such that the output is the same as the input for the training set. This might look useless, but the relevant aspect is that the model is trained to encode the input to a bottleneck, such that it is sufficient to faithfullt reconstruct the input values in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8bd17d-b29e-4271-9112-21ea1acc4858",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "\n",
    "We'll use\n",
    "* time -- a standard Python library providing time related functions\n",
    "* matplotlib for showing images -- more info in https://matplotlib.org/\n",
    "* PyTorch is a machine learning library -- more info in https://pytorch.org/\n",
    "  * torch.nn for using neural networks\n",
    "  * torch.utils.data for loading datasets\n",
    "  * torchvision.datasets for access to the EuroSAT dataset\n",
    "  * torchvision.transforms for data transformation among images and tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f49ce7-6f3e-4bd3-99bb-d73a854d0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import *\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547666e-3a8f-46f2-9348-b8f2a92b43be",
   "metadata": {},
   "source": [
    "Determination of the AI acceleration device, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f2f8ee-48b4-47d5-a848-19b5c7bd4de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b7f5c-a30d-490b-bc08-178771b57c09",
   "metadata": {},
   "source": [
    "## Dataset loading\n",
    "\n",
    "Next we load the MNIST dataset. We'll use the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5488a756-f1c7-43a5-9144-d324f4164b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d0b60a-299f-4f3a-be27-f1bf6fa4b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Create data loader\n",
    "dataloader = DataLoader(full_dataset, batch_size=batch_size)\n",
    "\n",
    "# Show the shape of one instance -- we'll only use the image for training\n",
    "for X, y in dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb874df-ede0-4d70-ba4b-05ceb9d98dd2",
   "metadata": {},
   "source": [
    "Next we show some examples of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c8ea7-4d8a-4d6b-9455-581ba1865502",
   "metadata": {},
   "outputs": [],
   "source": [
    "(fig,ax) = plt.subplots(1, 10, constrained_layout=True)\n",
    "ds = full_dataset\n",
    "for j in range(10):\n",
    "    ax[j].imshow(ds[j][0][0], cmap=\"gray\")\n",
    "    ax[j].set_axis_off()\n",
    "    ax[j].set_title(ds[j][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efab5707-c52b-42c3-a104-8a8e1b64bf51",
   "metadata": {},
   "source": [
    "## Create a neural network model\n",
    "\n",
    "Here we define some models to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c3a115-f2e8-41f0-a24d-7f59e07f7248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicAutoencoderMNIST(nn.Module):\n",
    "    bottleneck = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.unflatten = nn.Unflatten(1, (1,28,28))\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.bottleneck)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.flatten(x)\n",
    "        coded = self.encoder(x)\n",
    "        return coded\n",
    "\n",
    "    def decode(self, x):\n",
    "        x = self.decoder(x)\n",
    "        decoded = self.unflatten(x)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode( self.encode(x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0976556-9ea9-45c9-a1a4-9cec2405c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoderMNIST(nn.Module):\n",
    "    bottleneck = 10\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32*7*7, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, self.bottleneck)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.bottleneck, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32*7*7),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (32,7,7)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid() # Output should be in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        coded = self.encoder(x)\n",
    "        return coded\n",
    "\n",
    "    def decode(self, x):\n",
    "        decoded = self.decoder(x)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode( self.encode(x) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9513ba-03bf-4451-b9ff-0d2823c1536c",
   "metadata": {},
   "source": [
    "Selection of the model, the loss and the optimier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a4ab84-f731-4f14-986d-8c89d49765a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the model you want to try out, leaving all others commented out\n",
    "model = ClassicAutoencoderMNIST().to(device)\n",
    "#model = ConvAutoencoderMNIST().to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb39c6f-b717-4eb6-bfb6-5f31f89b3eb6",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "First we define a train function. Differently than previously, the model is trained to minimize the difference (error) between the output and input, over the training set. An encoding will automatically emerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5a69d5-4d56-4ac9-9bc6-21f854d912a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    t = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, X)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if time.monotonic()-t > 1:\n",
    "            t = time.monotonic()\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0c019-2443-428c-acbd-a3fd6fef6616",
   "metadata": {},
   "source": [
    "The next cell performs the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7afb773-8fd9-414a-ab9f-f47b27bb8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43771e6-f6e4-4bb4-8f62-700710d1fe88",
   "metadata": {},
   "source": [
    "The following two cells can be used to save the model to disk or load it from disk. __Skip__ these two cells, unless you wish to save and/or load models to/from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52460cc-087f-4b2e-91fa-2312b06e52c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6a959a-2935-457d-ac3b-d7fa090ed669",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b77f0-2449-4683-887c-fb1ec266fa26",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "\n",
    "The next cell shows the encoding and decoding for several instances of the dataset.\n",
    "* first row: input\n",
    "* second row: encoded input at the bottleneck\n",
    "* third row: output, reconstructing the input\n",
    "\n",
    "Note how well the input is reconstructed, even though the encoded data is much smaller than the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c7e52-8fff-4bf6-9b87-20807f58ffa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "(fig,ax) = plt.subplots(3, 10, constrained_layout=True)\n",
    "ds = full_dataset\n",
    "for j in range(10):\n",
    "    x, _ = ds[j]\n",
    "    x = x[None,:].to(device)\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x)\n",
    "        y = model.decode(z)\n",
    "    \n",
    "    ax[0,j].imshow(ds[j][0][0], cmap=\"gray\")\n",
    "    ax[0,j].set_axis_off()\n",
    "    ax[1,j].imshow(z[None,0].cpu(), cmap=\"gray\")\n",
    "    ax[1,j].set_axis_off()\n",
    "    ax[2,j].imshow(y[0][0].cpu(), cmap=\"gray\")\n",
    "    ax[2,j].set_axis_off()\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ac50a-9680-43c8-b847-ae6c9782fb62",
   "metadata": {},
   "source": [
    "An autoencoder can be seen as a rudimentary __generative model__, in the sense that the decoder generates instances similar to the ones in the training set.\n",
    "\n",
    "To demonstrate this feature, we show next the output of randomly generated values in the bottleneck. Althought the output images are not quite handwritten digits, they somehow resemble them.\n",
    "\n",
    "The first row shows the values at the bottleneck, while the second row shows the corresponding generated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8411c2f-8463-4ec9-a2e6-f4eea8b1de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "(fig,ax) = plt.subplots(2, 10, constrained_layout=True)\n",
    "for j in range(10):\n",
    "    z = torch.randn(10)\n",
    "    z = z[None,:].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y = model.decode(z)\n",
    "    \n",
    "    ax[0,j].imshow(z[None,0].cpu(), cmap=\"gray\")\n",
    "    ax[0,j].set_axis_off()\n",
    "    ax[1,j].imshow(y[0][0].cpu(), cmap=\"gray\")\n",
    "    ax[1,j].set_axis_off()\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f13e26-a0f0-4ee7-a22a-ddc46e82ecda",
   "metadata": {},
   "source": [
    "Now let's play a little bit with the decoder.\n",
    "\n",
    "First we compute the mean and covariance of the encoded values at the bottleneck, over the dataset. Intuitively, the mean is the \"center of mass\" of the encoded values, while the covariance represents the dispersion of those values.\n",
    "\n",
    "Then we perform the spectral decomposition (i.e., compute the eigenvalues and eigenvectors) of the covariance matrix. This is effectively the same as the Principal Component Analysis (PCA) of the encoded values. The principal components is a set of orthogonal directions over which the data is dispersed. This allows the identification of the main directions over which the data varies around its \"center of mass\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca9614-44b4-4470-a6ad-e442a488ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = torch.stack([x for (x,_) in full_dataset])\n",
    "all_data = all_data.to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model.encode(all_data)\n",
    "    out = out.cpu()\n",
    "\n",
    "# determine mean and covariance of encoded data\n",
    "u = torch.mean(out,0)\n",
    "S = torch.cov(out.T)\n",
    "\n",
    "# perform spectral decompositon of the covariance matrix, i.e., Principal Component Analysis (PCA)\n",
    "(L,Q)=torch.linalg.eigh(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388adf9c-64ea-40b0-8115-c015561bec7f",
   "metadata": {},
   "source": [
    "Here we take the two principal components with highest dispersion, and decode the values along these twp components, along rows and columns below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c079cc-d83e-435c-a2f3-8527fd616844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "(fig,ax) = plt.subplots(11, 11, constrained_layout=True)\n",
    "for i in range(11):\n",
    "    for j in range(11):\n",
    "        z = u + (i-5)*torch.sqrt(L[-2])*Q[:,-2] \\\n",
    "              + (j-5)*torch.sqrt(L[-1])*Q[:,-1]\n",
    "        z = z[None,:].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            y = model.decode(z)\n",
    "        \n",
    "        ax[i,j].imshow(y[0][0].cpu(), cmap=\"gray\")\n",
    "        ax[i,j].set_axis_off()\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b0b679-0418-401d-a79d-1663d60714ee",
   "metadata": {},
   "source": [
    "Now let's range over each one of the 10 highest principal components, one at a time, and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29286dc-8d2a-4eab-8949-1cfa7dd8c960",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate some images along the 10 highest principal values\n",
    "\n",
    "model.eval()\n",
    "(fig,ax) = plt.subplots(10, 11, constrained_layout=True)\n",
    "for i in range(10):\n",
    "    for j in range(11):\n",
    "        z = u + (j-5)*torch.sqrt(L[-1-i])*Q[:,-1-i]\n",
    "        z = z[None,:].to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            y = model.decode(z)\n",
    "        \n",
    "        ax[i,j].imshow(y[0][0].cpu(), cmap=\"gray\")\n",
    "        ax[i,j].set_axis_off()\n",
    "    \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f384e5e-0d49-4b7f-981b-146ef91421bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
